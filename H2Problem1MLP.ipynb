{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOV2dYWygs3deBUIci4cdoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britt-klose/GenAI-HW2/blob/main/H2Problem1MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brittany Klose GENAI HW-2"
      ],
      "metadata": {
        "id": "omAB3YovRLrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1:** **Data** **Preprocessing**\n",
        "\n",
        "* Load the CIFAR-10 dataset using the appropriate function from tf.keras.datasets.\n",
        "\n",
        "* Normalize the images by dividing by 255.0."
      ],
      "metadata": {
        "id": "tXgPAMuCJp2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w46uQOSLJiC9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers, utils, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "NUM_CLASSES = 10"
      ],
      "metadata": {
        "id": "2bx6x2PxNRCF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare the Data\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipQSOeGXNVIu",
        "outputId": "88346f63-3cce-4e31-9931-18806b33474b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "y_train = utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "metadata": {
        "id": "12D5oZLMN5pe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(x_train[:10])\n",
        "print(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EIML2mUbkSV4",
        "outputId": "2415c66f-c669-4ed7-c6fe-b2c913bd9411"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[[[0.00090734, 0.00095348, 0.00096886],\n",
              "         [0.00066128, 0.00070742, 0.00069204],\n",
              "         [0.00076894, 0.00073818, 0.00066128],\n",
              "         ...,\n",
              "         [0.00242983, 0.00202999, 0.0016609 ],\n",
              "         [0.00233756, 0.00192234, 0.00156863],\n",
              "         [0.00227605, 0.00190696, 0.00158401]],\n",
              "\n",
              "        [[0.00024606, 0.00030757, 0.00030757],\n",
              "         [0.        , 0.        , 0.        ],\n",
              "         [0.00027682, 0.00012303, 0.        ],\n",
              "         ...,\n",
              "         [0.00189158, 0.00135333, 0.00084583],\n",
              "         [0.00183007, 0.00127643, 0.00076894],\n",
              "         [0.0018762 , 0.00133795, 0.00087659]],\n",
              "\n",
              "        [[0.00038447, 0.00036909, 0.00032295],\n",
              "         [0.00024606, 0.00010765, 0.        ],\n",
              "         [0.00075356, 0.00041522, 0.00012303],\n",
              "         ...,\n",
              "         [0.00181469, 0.00129181, 0.00076894],\n",
              "         [0.00184544, 0.00129181, 0.00076894],\n",
              "         [0.00167628, 0.00112265, 0.00064591]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00319877, 0.00261438, 0.00147636],\n",
              "         [0.00309112, 0.00235294, 0.00052288],\n",
              "         [0.00304498, 0.00247597, 0.00039985],\n",
              "         ...,\n",
              "         [0.00246059, 0.00204537, 0.00107651],\n",
              "         [0.00086121, 0.00047674, 0.00010765],\n",
              "         [0.00081507, 0.00052288, 0.00030757]],\n",
              "\n",
              "        [[0.00276817, 0.00213764, 0.00147636],\n",
              "         [0.00266052, 0.00189158, 0.00064591],\n",
              "         [0.00286044, 0.00221453, 0.00046136],\n",
              "         ...,\n",
              "         [0.00282968, 0.00227605, 0.0014456 ],\n",
              "         [0.00149173, 0.00095348, 0.00052288],\n",
              "         [0.00127643, 0.00081507, 0.00052288]],\n",
              "\n",
              "        [[0.00272203, 0.00221453, 0.00178393],\n",
              "         [0.00258362, 0.00198385, 0.0014456 ],\n",
              "         [0.00275279, 0.00218378, 0.00133795],\n",
              "         ...,\n",
              "         [0.0033218 , 0.00282968, 0.00215302],\n",
              "         [0.00232218, 0.00181469, 0.00129181],\n",
              "         [0.00189158, 0.00141484, 0.00110727]]],\n",
              "\n",
              "\n",
              "       [[[0.00236832, 0.00272203, 0.00287582],\n",
              "         [0.00193772, 0.00210688, 0.0020915 ],\n",
              "         [0.00161476, 0.00159938, 0.00146098],\n",
              "         ...,\n",
              "         [0.00139946, 0.00146098, 0.00109189],\n",
              "         [0.00133795, 0.00138408, 0.00109189],\n",
              "         [0.00121492, 0.00124567, 0.00107651]],\n",
              "\n",
              "        [[0.00215302, 0.00246059, 0.002599  ],\n",
              "         [0.00222991, 0.00235294, 0.00236832],\n",
              "         [0.00192234, 0.00192234, 0.00181469],\n",
              "         ...,\n",
              "         [0.00147636, 0.00152249, 0.00119954],\n",
              "         [0.00118416, 0.0012303 , 0.00095348],\n",
              "         [0.00109189, 0.00112265, 0.0009381 ]],\n",
              "\n",
              "        [[0.00215302, 0.0023837 , 0.00252211],\n",
              "         [0.00213764, 0.00224529, 0.00229143],\n",
              "         [0.00176855, 0.00176855, 0.00172241],\n",
              "         ...,\n",
              "         [0.00121492, 0.00126105, 0.00098424],\n",
              "         [0.00104575, 0.00107651, 0.00084583],\n",
              "         [0.00103037, 0.00106113, 0.00084583]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00269127, 0.00256824, 0.00255286],\n",
              "         [0.00239908, 0.00236832, 0.00246059],\n",
              "         [0.00236832, 0.00246059, 0.00261438],\n",
              "         ...,\n",
              "         [0.00064591, 0.00052288, 0.00055363],\n",
              "         [0.0009381 , 0.00081507, 0.00087659],\n",
              "         [0.00143022, 0.00127643, 0.00139946]],\n",
              "\n",
              "        [[0.00253749, 0.00236832, 0.00196847],\n",
              "         [0.00239908, 0.00233756, 0.00199923],\n",
              "         [0.00244521, 0.00247597, 0.00218378],\n",
              "         ...,\n",
              "         [0.00158401, 0.00143022, 0.00147636],\n",
              "         [0.00189158, 0.00175317, 0.00184544],\n",
              "         [0.00201461, 0.00186082, 0.00201461]],\n",
              "\n",
              "        [[0.00250673, 0.00227605, 0.00184544],\n",
              "         [0.00242983, 0.00227605, 0.0018762 ],\n",
              "         [0.00250673, 0.00239908, 0.00204537],\n",
              "         ...,\n",
              "         [0.00219915, 0.00204537, 0.00213764],\n",
              "         [0.00219915, 0.00206075, 0.00218378],\n",
              "         [0.00219915, 0.00204537, 0.00221453]]],\n",
              "\n",
              "\n",
              "       [[[0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00389081, 0.00389081, 0.00389081],\n",
              "         [0.00389081, 0.00389081, 0.00389081],\n",
              "         ...,\n",
              "         [0.00389081, 0.00389081, 0.00389081],\n",
              "         [0.00389081, 0.00389081, 0.00389081],\n",
              "         [0.00389081, 0.00389081, 0.00389081]],\n",
              "\n",
              "        [[0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00392157, 0.00392157, 0.00392157],\n",
              "         ...,\n",
              "         [0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00392157, 0.00392157, 0.00392157]],\n",
              "\n",
              "        [[0.00392157, 0.00392157, 0.00392157],\n",
              "         [0.00390619, 0.00390619, 0.00390619],\n",
              "         [0.00390619, 0.00390619, 0.00390619],\n",
              "         ...,\n",
              "         [0.00390619, 0.00390619, 0.00390619],\n",
              "         [0.00390619, 0.00390619, 0.00390619],\n",
              "         [0.00390619, 0.00390619, 0.00390619]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00173779, 0.00184544, 0.00172241],\n",
              "         [0.00170704, 0.00181469, 0.00170704],\n",
              "         [0.00161476, 0.00172241, 0.00163014],\n",
              "         ...,\n",
              "         [0.00110727, 0.00124567, 0.0012303 ],\n",
              "         [0.00110727, 0.0012303 , 0.00121492],\n",
              "         [0.00110727, 0.0012303 , 0.00121492]],\n",
              "\n",
              "        [[0.00170704, 0.00181469, 0.00169166],\n",
              "         [0.00159938, 0.00170704, 0.00159938],\n",
              "         [0.00152249, 0.00163014, 0.00150711],\n",
              "         ...,\n",
              "         [0.00104575, 0.0011534 , 0.00112265],\n",
              "         [0.00107651, 0.00116878, 0.0011534 ],\n",
              "         [0.00119954, 0.00129181, 0.00126105]],\n",
              "\n",
              "        [[0.00163014, 0.00173779, 0.00161476],\n",
              "         [0.00152249, 0.00163014, 0.00150711],\n",
              "         [0.00146098, 0.00156863, 0.0014456 ],\n",
              "         ...,\n",
              "         [0.00119954, 0.00130719, 0.00127643],\n",
              "         [0.00121492, 0.00130719, 0.00127643],\n",
              "         [0.0012303 , 0.00132257, 0.00129181]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.0004306 , 0.00053825, 0.00059977],\n",
              "         [0.00046136, 0.00052288, 0.00067666],\n",
              "         [0.0005075 , 0.00067666, 0.0007228 ],\n",
              "         ...,\n",
              "         [0.00066128, 0.00086121, 0.00069204],\n",
              "         [0.00079969, 0.00098424, 0.00081507],\n",
              "         [0.00070742, 0.00089196, 0.0007228 ]],\n",
              "\n",
              "        [[0.00041522, 0.00046136, 0.00058439],\n",
              "         [0.00041522, 0.0004306 , 0.00063053],\n",
              "         [0.00032295, 0.00047674, 0.00059977],\n",
              "         ...,\n",
              "         [0.00172241, 0.0020915 , 0.00149173],\n",
              "         [0.00179931, 0.00215302, 0.00155325],\n",
              "         [0.00176855, 0.00212226, 0.00153787]],\n",
              "\n",
              "        [[0.00052288, 0.00055363, 0.00064591],\n",
              "         [0.0005075 , 0.0005075 , 0.00066128],\n",
              "         [0.00036909, 0.00046136, 0.00061515],\n",
              "         ...,\n",
              "         [0.00269127, 0.00319877, 0.00219915],\n",
              "         [0.00272203, 0.00321415, 0.00221453],\n",
              "         [0.00270665, 0.00319877, 0.00219915]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00218378, 0.00270665, 0.00181469],\n",
              "         [0.00218378, 0.00270665, 0.00181469],\n",
              "         [0.00230681, 0.00282968, 0.00195309],\n",
              "         ...,\n",
              "         [0.00206075, 0.00269127, 0.00183007],\n",
              "         [0.00196847, 0.00258362, 0.00172241],\n",
              "         [0.00206075, 0.00269127, 0.00183007]],\n",
              "\n",
              "        [[0.00215302, 0.00270665, 0.00190696],\n",
              "         [0.00222991, 0.00276817, 0.00198385],\n",
              "         [0.00230681, 0.00286044, 0.00206075],\n",
              "         ...,\n",
              "         [0.00201461, 0.00261438, 0.00183007],\n",
              "         [0.00199923, 0.00261438, 0.00183007],\n",
              "         [0.0018762 , 0.00249135, 0.00170704]],\n",
              "\n",
              "        [[0.00206075, 0.00262976, 0.00189158],\n",
              "         [0.0020915 , 0.00262976, 0.00190696],\n",
              "         [0.0020915 , 0.00262976, 0.00190696],\n",
              "         ...,\n",
              "         [0.00163014, 0.00221453, 0.00153787],\n",
              "         [0.00159938, 0.00218378, 0.00152249],\n",
              "         [0.00155325, 0.00215302, 0.00147636]]],\n",
              "\n",
              "\n",
              "       [[[0.00206075, 0.00286044, 0.00342945],\n",
              "         [0.00201461, 0.00282968, 0.00338331],\n",
              "         [0.00196847, 0.00279892, 0.00335256],\n",
              "         ...,\n",
              "         [0.00195309, 0.00278354, 0.00341407],\n",
              "         [0.00195309, 0.00278354, 0.00341407],\n",
              "         [0.00196847, 0.00279892, 0.00342945]],\n",
              "\n",
              "        [[0.00204537, 0.00290657, 0.00350634],\n",
              "         [0.00198385, 0.00286044, 0.00344483],\n",
              "         [0.00196847, 0.00286044, 0.00344483],\n",
              "         ...,\n",
              "         [0.00195309, 0.0028143 , 0.00344483],\n",
              "         [0.00195309, 0.0028143 , 0.00344483],\n",
              "         [0.00196847, 0.00282968, 0.00346021]],\n",
              "\n",
              "        [[0.00196847, 0.00284506, 0.00347559],\n",
              "         [0.00195309, 0.00279892, 0.00342945],\n",
              "         [0.00196847, 0.00279892, 0.00342945],\n",
              "         ...,\n",
              "         [0.00193772, 0.00278354, 0.00341407],\n",
              "         [0.00193772, 0.00278354, 0.00341407],\n",
              "         [0.00193772, 0.00276817, 0.00339869]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00267589, 0.00319877, 0.00361399],\n",
              "         [0.00262976, 0.00316801, 0.00352172],\n",
              "         [0.0023837 , 0.00290657, 0.0033218 ],\n",
              "         ...,\n",
              "         [0.00041522, 0.0014456 , 0.0020915 ],\n",
              "         [0.00044598, 0.00147636, 0.00210688],\n",
              "         [0.0004306 , 0.0014456 , 0.0020915 ]],\n",
              "\n",
              "        [[0.00298347, 0.00339869, 0.0037524 ],\n",
              "         [0.00290657, 0.00330642, 0.00367551],\n",
              "         [0.00244521, 0.00301423, 0.00346021],\n",
              "         ...,\n",
              "         [0.00046136, 0.00146098, 0.00212226],\n",
              "         [0.00046136, 0.00147636, 0.00213764],\n",
              "         [0.00046136, 0.00146098, 0.00215302]],\n",
              "\n",
              "        [[0.00296809, 0.00333718, 0.00364475],\n",
              "         [0.00278354, 0.00319877, 0.0035371 ],\n",
              "         [0.00258362, 0.00309112, 0.00349097],\n",
              "         ...,\n",
              "         [0.00047674, 0.0014456 , 0.0020915 ],\n",
              "         [0.00049212, 0.0014456 , 0.00210688],\n",
              "         [0.00049212, 0.0014456 , 0.00212226]]],\n",
              "\n",
              "\n",
              "       [[[0.00192234, 0.00192234, 0.00178393],\n",
              "         [0.00169166, 0.00155325, 0.00139946],\n",
              "         [0.00156863, 0.00138408, 0.00127643],\n",
              "         ...,\n",
              "         [0.0031065 , 0.00318339, 0.00329104],\n",
              "         [0.00307574, 0.00315263, 0.00326028],\n",
              "         [0.0031065 , 0.00319877, 0.00329104]],\n",
              "\n",
              "        [[0.00218378, 0.00224529, 0.00218378],\n",
              "         [0.00224529, 0.00221453, 0.00213764],\n",
              "         [0.00270665, 0.00264514, 0.00261438],\n",
              "         ...,\n",
              "         [0.00299885, 0.00309112, 0.00315263],\n",
              "         [0.00304498, 0.00315263, 0.00321415],\n",
              "         [0.00313725, 0.00324491, 0.00330642]],\n",
              "\n",
              "        [[0.00276817, 0.00284506, 0.0028143 ],\n",
              "         [0.00219915, 0.00224529, 0.00224529],\n",
              "         [0.00239908, 0.00241446, 0.00241446],\n",
              "         ...,\n",
              "         [0.0018762 , 0.00170704, 0.00173779],\n",
              "         [0.00213764, 0.00196847, 0.00201461],\n",
              "         [0.00242983, 0.00226067, 0.00230681]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00159938, 0.00126105, 0.00063053],\n",
              "         [0.00155325, 0.0012303 , 0.00059977],\n",
              "         [0.00155325, 0.00124567, 0.00058439],\n",
              "         ...,\n",
              "         [0.00193772, 0.00158401, 0.00103037],\n",
              "         [0.00193772, 0.00158401, 0.00106113],\n",
              "         [0.00192234, 0.00155325, 0.00104575]],\n",
              "\n",
              "        [[0.00159938, 0.00124567, 0.00061515],\n",
              "         [0.00161476, 0.00129181, 0.00063053],\n",
              "         [0.00167628, 0.00135333, 0.00066128],\n",
              "         ...,\n",
              "         [0.00212226, 0.00173779, 0.00119954],\n",
              "         [0.00210688, 0.00173779, 0.0012303 ],\n",
              "         [0.00210688, 0.00172241, 0.00124567]],\n",
              "\n",
              "        [[0.00161476, 0.00127643, 0.00064591],\n",
              "         [0.0016609 , 0.00133795, 0.00069204],\n",
              "         [0.00176855, 0.0014456 , 0.00076894],\n",
              "         ...,\n",
              "         [0.00219915, 0.00179931, 0.00126105],\n",
              "         [0.00219915, 0.00178393, 0.00129181],\n",
              "         [0.00221453, 0.00178393, 0.00132257]]]], dtype=float32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2:** **MLP Model**\n",
        "\n",
        "* Declare the function: **def create_mlp_model(input_shape)**\n",
        "*  Have the function create an **MLP model** using **Sequential** and add layers using **Flatten** and **Dense** from *tf.keras.layers*.\n",
        "* **Add Dropout layers:** Dropout helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time, which helps the model generalize better.\n",
        "* Compile the MLP model with an optimizer and loss function (**adam** and **sparse_categorical_crossentropy**).\n",
        "* Train the MLP model using the **fit **method and include validation split."
      ],
      "metadata": {
        "id": "15AJldgnRYRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4:** **Evaluation**\n",
        "\n",
        "* Evaluate both models on the test set using the *evaluate* method."
      ],
      "metadata": {
        "id": "TLVG16HFTigN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5:** **Plotting Results**\n",
        "\n",
        "* **Plot the validation accuracy** for both models using *matplotlib.pyplot.*\n",
        "* **Plot the training accuracy** for both models using *matplotlib.pyplot.*"
      ],
      "metadata": {
        "id": "OGXR8e1-Tvcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MLP Model\n",
        "\n",
        "#################################################\n",
        "#          2. Building the model\n",
        "#################################################\n",
        "def create_mlp_model():\n",
        "\n",
        "  # Using Sequential model\n",
        "  model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(32, 32, 3)),\n",
        "    layers.Dense(200, activation='relu'),\n",
        "    layers.Dense(150, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "    layers.Dropout(rate=0.5)\n",
        "  ])\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "  opt = optimizers.Adam(learning_rate=0.0005)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  #####################################################\n",
        "  #         Training the Model with .fit()\n",
        "  #####################################################\n",
        "  model.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n",
        "\n",
        "\n",
        "#####################################################\n",
        "#        4. Evaluating the Model\n",
        "#####################################################\n",
        "  model.evaluate(x_test, y_test)\n",
        "\n",
        "  CLASSES = np.array(\n",
        "      [\n",
        "          \"airplane\",\n",
        "          \"automobile\",\n",
        "          \"bird\",\n",
        "          \"cat\",\n",
        "          \"deer\",\n",
        "          \"dog\",\n",
        "          \"frog\",\n",
        "          \"horse\",\n",
        "          \"ship\",\n",
        "          \"truck\",\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  preds = model.predict(x_test)\n",
        "  preds_single = CLASSES[np.argmax(preds, axis=-1)]\n",
        "  actual_single = CLASSES[np.argmax(y_test, axis=-1)]\n",
        "\n",
        "\n",
        "#####################################################\n",
        "#        5. Plotting the Model\n",
        "#####################################################\n",
        "\n",
        "  n_to_show = 10\n",
        "  indices = np.random.choice(range(len(x_test)), n_to_show)\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 3))\n",
        "  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "  for i, idx in enumerate(indices):\n",
        "      img = x_test[idx]\n",
        "      ax = fig.add_subplot(1, n_to_show, i + 1)\n",
        "      ax.axis(\"off\")\n",
        "      ax.text(\n",
        "          0.5,\n",
        "          -0.35,\n",
        "          \"pred = \" + str(preds_single[idx]),\n",
        "          fontsize=10,\n",
        "          ha=\"center\",\n",
        "          transform=ax.transAxes,\n",
        "      )\n",
        "      ax.text(\n",
        "          0.5,\n",
        "          -0.7,\n",
        "          \"act = \" + str(actual_single[idx]),\n",
        "          fontsize=10,\n",
        "          ha=\"center\",\n",
        "          transform=ax.transAxes,\n",
        "      )\n",
        "      ax.imshow(img)\n"
      ],
      "metadata": {
        "id": "3Nue7YgqRWGa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_mlp_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n3jLrdulnLVk",
        "outputId": "8d6004ce-74c1-45dc-9d2a-576379230810"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               614600    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 150)               30150     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1510      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 646260 (2.47 MB)\n",
            "Trainable params: 646260 (2.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 10, 10) and (None, 10) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-91cfca26dad8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_mlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-db84dbaf02d9>\u001b[0m in \u001b[0;36mcreate_mlp_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m#         Training the Model with .fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m#####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 10, 10) and (None, 10) are incompatible\n"
          ]
        }
      ]
    }
  ]
}